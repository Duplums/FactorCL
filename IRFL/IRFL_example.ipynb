{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FactorCL on IRFL"
      ],
      "metadata": {
        "id": "LTEV_elPBK32"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWNBW_QzYrvs"
      },
      "source": [
        "##Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgGoqg0DYP92",
        "outputId": "8db4a419-7304-4ecc-b552-b0c44344c123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MultiBench'...\n",
            "remote: Enumerating objects: 6925, done.\u001b[K\n",
            "remote: Counting objects: 100% (136/136), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 6925 (delta 62), reused 123 (delta 60), pack-reused 6789\u001b[K\n",
            "Receiving objects: 100% (6925/6925), 51.06 MiB | 17.93 MiB/s, done.\n",
            "Resolving deltas: 100% (4248/4248), done.\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1KGIlFvYqYH",
        "outputId": "f3a508b7-669f-4a5d-b9c7-45a65a53f641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MultiBench\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/irfl-dataset/IRFL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3RpE80CYxIY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "\n",
        "import PIL.Image as Image\n",
        "import requests\n",
        "from urllib.request import urlopen"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pliang279/FactorCL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHKnbVkoASPI",
        "outputId": "8e0fe866-1205-4a2c-df8e-d12e89123883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FactorCL'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 37 (delta 14), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (37/37), 20.62 KiB | 1.87 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd FactorCL"
      ],
      "metadata": {
        "id": "pyWpONMApjAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoProcessor, CLIPModel\n",
        "\n",
        "from IRFL_model import*"
      ],
      "metadata": {
        "id": "A0mgHF3bAgvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##IRFL Dataset"
      ],
      "metadata": {
        "id": "AVfFndkOl_Tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simile_df = pd.read_csv('/content/IRFL/assets/tasks/simile_understanding_task.csv')\n",
        "idiom_df = pd.read_csv('/content/IRFL/assets/tasks/idiom_understanding_task.csv')\n",
        "metaphor_df = pd.read_csv('/content/IRFL/assets/tasks/metaphor_understanding_task.csv')"
      ],
      "metadata": {
        "id": "p8cqWO_Ul9BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_df(df):\n",
        "  distractors_urls = df['distractors'].to_list()\n",
        "  answers_urls = df['distractors'].to_list()\n",
        "  phrases = df['phrase'].to_list()\n",
        "  fig_types = df['figurative_type'].to_list()\n",
        "\n",
        "  distractors = []\n",
        "  answers = []\n",
        "  texts = []\n",
        "  types = []\n",
        "\n",
        "  for i in range(len(distractors_urls)):\n",
        "    print(f'{i}/{len(distractors_urls)}')\n",
        "    try:\n",
        "      d_urls = distractors_urls[i]\n",
        "      distractor = [Image.open(urlopen(url)) for url in eval(d_urls)]\n",
        "\n",
        "      a_urls = answers_urls[i]\n",
        "      answer = Image.open(urlopen(eval(a_urls)[0]))\n",
        "\n",
        "      text = phrases[i]\n",
        "      fig_type = fig_types[i]\n",
        "      \n",
        "      distractors.append(distractor)\n",
        "      answers.append(answer)\n",
        "      texts.append(text)\n",
        "      types.append(fig_type)\n",
        "    except:\n",
        "      continue\n",
        "  \n",
        "  return distractors, answers, texts, types\n",
        "\n",
        "def collate_fn(batch):\n",
        "    #return torch.cat([data[0] for data in batch]), torch.stack([data[1] for data in batch])\n",
        "\n",
        "    images = [data[0] for data in batch]\n",
        "    texts = [data[1] for data in batch]\n",
        "    labels = [data[2] for data in batch]\n",
        "\n",
        "    return images, texts, torch.tensor(labels, dtype=int)\n",
        "\n",
        "def process_fn(batch):\n",
        "    images, texts, labels = batch\n",
        "    batch = processor(images=images, text=texts, padding=True, return_tensors='pt')\n",
        "\n",
        "    return batch, labels\n",
        "\n",
        "class FigTypeDataset(Dataset):\n",
        "    def __init__(self, answers, texts, types):\n",
        "        self.types= types\n",
        "        self.images = answers\n",
        "        self.texts = texts\n",
        "        \n",
        "        self.type_map = {'idiom': 0, 'simile': 1, 'metaphor': 2}\n",
        "\n",
        "        self.labels = list(map(lambda x: self.type_map[x], self.types))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.texts[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "l70txHYvl4rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeds(model, processor, train_loader, test_loader):\n",
        "    train_embeds = []\n",
        "    train_labels = []\n",
        "    test_embeds = []\n",
        "    test_labels = []\n",
        "    for i_batch, x in enumerate(train_loader):\n",
        "\n",
        "        inputs, label = process_fn(x)\n",
        "        inputs, label = inputs.to(device), label.to(device)\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        image_embeds = outputs.image_embeds.detach().cpu().numpy()\n",
        "        text_embeds = outputs.text_embeds.detach().cpu().numpy()\n",
        "\n",
        "        embeds = np.concatenate([image_embeds, text_embeds], axis=1)\n",
        "        train_embeds.append(embeds)\n",
        "        train_labels.append(label.detach().cpu().numpy())\n",
        "\n",
        "    for i_batch, x in enumerate(test_loader):\n",
        "\n",
        "        inputs, label = process_fn(x)\n",
        "        inputs, label = inputs.to(device), label.to(device)\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        image_embeds = outputs.image_embeds.detach().cpu().numpy()\n",
        "        text_embeds = outputs.text_embeds.detach().cpu().numpy()\n",
        "\n",
        "        embeds = np.concatenate([image_embeds, text_embeds], axis=1)\n",
        "        test_embeds.append(embeds)\n",
        "        test_labels.append(label.detach().cpu().numpy())\n",
        "\n",
        "    train_embeds = np.concatenate(train_embeds, axis=0)\n",
        "    test_embeds = np.concatenate(test_embeds, axis=0)\n",
        "    train_labels = np.concatenate(train_labels, axis=0)\n",
        "    test_labels = np.concatenate(test_labels, axis=0)\n",
        "\n",
        "    return train_embeds, train_labels, test_embeds, test_labels"
      ],
      "metadata": {
        "id": "-xCFTuq-p_Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distractors_simile, answers_simile, texts_simile, types_simile = process_df(simile_df)\n",
        "distractors_idiom, answers_idiom, texts_idiom, types_idiom = process_df(idiom_df)\n",
        "distractors_metaphor, answers_metaphor, texts_metaphor, types_metaphor = process_df(metaphor_df)"
      ],
      "metadata": {
        "id": "wm2643lupH0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distractors = distractors_idiom + distractors_simile + distractors_metaphor\n",
        "answers = answers_idiom + answers_simile + answers_metaphor\n",
        "texts = texts_idiom + texts_simile + texts_metaphor\n",
        "types = types_idiom + types_simile + types_metaphor"
      ],
      "metadata": {
        "id": "tdzj1UEjpNUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "dataset = FigTypeDataset(answers, texts, types)\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [int(0.8*len(dataset)), len(dataset)-int(0.8*len(dataset))])\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "IrBVVKsopsR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgpMeb4eRI5Z"
      },
      "source": [
        "##FactorCL-SUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha5JwFzHRKjd"
      },
      "outputs": [],
      "source": [
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "rus_model = RUSModel(model, processor, [512,512], 3, device, lr=1e-6).to(device)\n",
        "rus_model.train()\n",
        "\n",
        "train_rusmodel(rus_model, train_loader, num_epoch=10, num_club_iter=1)\n",
        "\n",
        "model.eval()\n",
        "train_embeds, train_labels, test_embeds, test_labels = get_embeds(model, processor, train_loader, test_loader)\n",
        "\n",
        "clf = LogisticRegression(max_iter=200).fit(train_embeds, train_labels)\n",
        "score = clf.score(test_embeds, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score"
      ],
      "metadata": {
        "id": "oTEJEZ1wdE_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bvQGiz2nq_J"
      },
      "source": [
        "##SimCLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7cS_0F7IYnr"
      },
      "outputs": [],
      "source": [
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "simclr_model = SupConResNet(model, processor, 0.5, [512,512], [512,512]).to(device)\n",
        "simclr_model.train()\n",
        "\n",
        "optimizer = optim.Adam(simclr_model.parameters(), lr=1e-6)\n",
        "\n",
        "train_supcon(simclr_model, train_loader, optimizer, num_epoch=10)\n",
        "\n",
        "model.eval()\n",
        "train_embeds, train_labels, test_embeds, test_labels = get_embeds(model, processor, train_loader, test_loader)\n",
        "\n",
        "clf = LogisticRegression(max_iter=200).fit(train_embeds, train_labels)\n",
        "score = clf.score(test_embeds, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score"
      ],
      "metadata": {
        "id": "zxpn6JcEWQc1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}